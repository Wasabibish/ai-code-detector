{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning Similarity Model Notebook\n",
    "\n",
    "This notebook fine-tunes a similarity model to predict the similarity between code snippets (whether AI-generated or not).\n",
    "\n",
    "- **Optional**: The fine-tuned model has been uploaded to my Hugging Face hub (username: `wasabibish`), making this notebook optional to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, losses, SentenceTransformerTrainer, util\n",
    "from sentence_transformers.training_args import SentenceTransformerTrainingArguments as TrainingArguments\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a training dataframe\n",
    "dataset_training = dataset_df = pd.DataFrame(columns=[\"sentence1\", \"sentence2\", \"score\"])\n",
    "\n",
    "# the dataset containing a pair (humain/given code, llm generated code) and the plagiarism score\n",
    "dataset_training['sentence1'] = data['human_content']\n",
    "dataset_training['sentence2'] = data['llm_content']\n",
    "dataset_training['score'] = data['plagiarism_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset from the dataframe\n",
    "dataset = Dataset.from_pandas(dataset_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset to train and test\n",
    "dataset_dict = dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = dataset_dict['train']\n",
    "test_dataset = dataset_dict['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traininig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'distilbert/distilbert-base-uncased-finetuned-sst-2-english'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model (sentence similarity model)\n",
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an evaluator for the model\n",
    "evaluator = EmbeddingSimilarityEvaluator(test_dataset['sentence1'], test_dataset['sentence2'], test_dataset['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    max_steps=100,\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.2,\n",
    "    save_steps=50,\n",
    "    warmup_steps=150,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=20,\n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the loss that will be used to train the model\n",
    "loss = losses.CosineSimilarityLoss(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the trainer\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    evaluator=evaluator,\n",
    "    loss=loss\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model = trainer.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model on the test dataset\n",
    "evaluator(finetuned_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_vector_db(test_dataset, finetuned_model):\n",
    "    \"\"\"\n",
    "    Simulate a database of vectors for the test dataset\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    test_dataset: pd.DataFrame\n",
    "        The test dataset\n",
    "    finetuned_model: SentenceTransformer\n",
    "        The fine-tuned model\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    vector_db: dict\n",
    "        A dictionary containing the vectors and scores of the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    # Create an empty dictionary to store vectors and scores\n",
    "    vector_db = {}\n",
    "\n",
    "\n",
    "    for i in range(len(test_dataset)):\n",
    "        sentence1 = test_dataset['sentence1'][i]\n",
    "        sentence2 = test_dataset['sentence2'][i]\n",
    "        score = test_dataset['score'][i]\n",
    "        \n",
    "        # encode sentences (create the embeddings)\n",
    "        vector1 = finetuned_model.encode(sentence1)\n",
    "        vector2 = finetuned_model.encode(sentence2)\n",
    "\n",
    "        # save them with the metadata\n",
    "        vector_db[sentence1] = {\n",
    "            'vector': vector1,\n",
    "            'score': score\n",
    "        }\n",
    "\n",
    "        vector_db[sentence2] = {\n",
    "            'vector': vector2,\n",
    "            'score': score\n",
    "        }\n",
    "\n",
    "    return vector_db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db = populate_vector_db(test_dataset, finetuned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weighted_score(sentence1_vector, vector_db,  top_n=3):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate the weighted score of a given sentence based on the similarity scores \n",
    "    of the top_n most similar sentences in the vector_db\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    sentence1_vector: np.array\n",
    "        The vector of the sentence for which the weighted score should be calculated\n",
    "    vector_db: dict\n",
    "        A dictionary containing the vectors and scores of the test dataset\n",
    "    top_n: int\n",
    "        The number of most similar sentences to consider\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    weighted_score: float\n",
    "        The weighted score of the sentence\n",
    "    \"\"\"\n",
    "\n",
    "    similarity_scores = {}\n",
    "\n",
    "    # calculate similarity scores for each sentence in the vector_db\n",
    "    for sentence, data in vector_db.items():\n",
    "        vector = data['vector']\n",
    "        score = data['score']\n",
    "        similarity = util.pytorch_cos_sim(sentence1_vector, vector).item()\n",
    "        similarity_scores[sentence] = {\n",
    "            'similarity': similarity,\n",
    "            'score': score\n",
    "        }\n",
    "\n",
    "    # sort sentences based on similarity scores\n",
    "    sorted_similarity_scores = sorted(similarity_scores.items(), key=lambda x: x[1]['similarity'], reverse=True)\n",
    "    most_similar_sentences = sorted_similarity_scores[:top_n]\n",
    "\n",
    "    # calculate the weighted score (similarity * real score)\n",
    "    total_score = 0\n",
    "    total_similarity = 0\n",
    "    for sentence, data in most_similar_sentences:\n",
    "        similarity = data['similarity']\n",
    "        score = data['score']\n",
    "        total_score += similarity * score\n",
    "        total_similarity += similarity\n",
    "\n",
    "    weighted_score = total_score / total_similarity if total_similarity > 0 else 0\n",
    "\n",
    "    return weighted_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_all_weighted_scores(test_dataset, vector_db, finetuned_model):\n",
    "    \"\"\"\n",
    "    Calculate the weighted scores for all sentences in the test dataset\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    test_dataset: pd.DataFrame\n",
    "        The test dataset\n",
    "    vector_db: dict\n",
    "        A dictionary containing the vectors and scores of the test dataset\n",
    "    finetuned_model: SentenceTransformer\n",
    "        The fine-tuned model\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    weighted_scores: list\n",
    "        A list of weighted scores for all sentences in the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    weighted_scores = []\n",
    "\n",
    "    for i in range(len(test_dataset)):\n",
    "        sentence1 = test_dataset['sentence1'][i]\n",
    "\n",
    "        # embbed sentence\n",
    "        sentence1_vector = finetuned_model.encode(sentence1)\n",
    "\n",
    "        # ealculate weighted score\n",
    "        weighted_score = calculate_weighted_score(sentence1_vector, vector_db)\n",
    "        weighted_scores.append(weighted_score)\n",
    "\n",
    "    return weighted_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = calculate_all_weighted_scores(test_dataset, vector_db, finetuned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_performance(predictions, test_dataset):\n",
    "    \"\"\"\n",
    "    Evaluate the performance of the model based on the predictions and the true scores\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    predictions: list\n",
    "        The predicted scores\n",
    "    test_dataset: pd.DataFrame\n",
    "        The test dataset\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    mse: float\n",
    "        The mean squared error\n",
    "    r2: float\n",
    "        The r2 score\n",
    "    \"\"\"\n",
    "    true_scores = test_dataset['score']\n",
    "    mse = mean_squared_error(true_scores, predictions)\n",
    "    r2 = r2_score(true_scores, predictions)\n",
    "\n",
    "    return mse, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_performance(predictions, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# push the model to huggingface\n",
    "model_name = 'similarity-code-ai-generated'\n",
    "finetuned_model.push_to_hub(model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
